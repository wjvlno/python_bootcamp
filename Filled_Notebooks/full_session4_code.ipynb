{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Loy6sTLYmhqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pLEwfIKmhGy"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.datasets import fetch_california_housing, load_iris, load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "df_california = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "df_california['MedHouseVal'] = california.target\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df_iris['species'] = iris.target\n",
        "\n",
        "# Load the Titanic dataset\n",
        "titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df_titanic = pd.read_csv(titanic_url)\n",
        "\n",
        "# Display the first few rows of each dataset\n",
        "df_california.head(), df_iris.head(), df_titanic.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Pandas Operations"
      ],
      "metadata": {
        "id": "o5ZL3m3hmmWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouping and Aggregating"
      ],
      "metadata": {
        "id": "NZ1hDHdPmp2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping and Aggregating (Instruction)\n",
        "# Group data by 'AveRooms' and calculate the mean of 'MedHouseVal' and 'AveOccup'\n",
        "grouped = df_california.groupby('AveRooms').agg({'MedHouseVal': 'mean', 'AveOccup': 'mean'})\n",
        "print(\"Grouped and Aggregated Data:\\n\", grouped)\n",
        "\n",
        "# Grouping and Aggregating (Participant - on your own)\n",
        "# Group data by 'HouseAge' and calculate the mean of 'MedHouseVal' and 'AveRooms'\n",
        "# Instructions: Group the dataset by 'HouseAge' and calculate the mean of 'MedHouseVal' and 'AveRooms'.\n",
        "# Use the .groupby() method and .agg() function similar to the previous example.\n",
        "grouped_house_age = df_california.groupby('HouseAge').agg({'MedHouseVal': 'mean', 'AveRooms': 'mean'})\n",
        "print(\"Grouped by HouseAge and Aggregated Data:\\n\", grouped_house_age)\n"
      ],
      "metadata": {
        "id": "T0ihV3qsmsbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging DataFrames"
      ],
      "metadata": {
        "id": "xG7RhSa6mvyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging DataFrames (Instruction)\n",
        "# Create a hypothetical additional_info_df\n",
        "additional_info_df = pd.DataFrame({\n",
        "    'AveRooms': [2, 4, 6, 8],\n",
        "    'additional_info': ['info1', 'info2', 'info3', 'info4']\n",
        "})\n",
        "merged_df = pd.merge(df_california, additional_info_df, on='AveRooms', how='left')\n",
        "print(\"Merged DataFrame:\\n\", merged_df.head())\n",
        "\n",
        "# Merging DataFrames (Participant - on your own)\n",
        "# Create another hypothetical additional_info_df\n",
        "additional_info_df_participant = pd.DataFrame({\n",
        "    'HouseAge': [10, 20, 30, 40],\n",
        "    'additional_info': ['infoA', 'infoB', 'infoC', 'infoD']\n",
        "})\n",
        "merged_df_participant = pd.merge(df_california, additional_info_df_participant, on='HouseAge', how='left')\n",
        "print(\"Merged DataFrame (Participant):\\n\", merged_df_participant.head())\n"
      ],
      "metadata": {
        "id": "QzA9RoMYmxln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot Tables"
      ],
      "metadata": {
        "id": "UnFAS1PpmzaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Pivot Tables (Instruction)\n",
        "# Create a pivot table with 'HouseAge' as index, 'AveRooms' as columns, and mean of 'MedHouseVal' as values\n",
        "pivot_table = pd.pivot_table(df_california, values='MedHouseVal', index='HouseAge', columns='AveRooms', aggfunc='mean')\n",
        "print(\"Pivot Table:\\n\", pivot_table)\n",
        "\n",
        "# Using Pivot Tables (Participant - on your own)\n",
        "# Create a pivot table with 'AveRooms' as index, 'HouseAge' as columns, and mean of 'MedHouseVal' as values\n",
        "pivot_table_participant = pd.pivot_table(df_california, values='MedHouseVal', index='AveRooms', columns='HouseAge', aggfunc='mean')\n",
        "print(\"Pivot Table (Participant):\\n\", pivot_table_participant)\n"
      ],
      "metadata": {
        "id": "HDdgjWWim1Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Machine Learning"
      ],
      "metadata": {
        "id": "Z1lP37jPm2ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and expecting the data"
      ],
      "metadata": {
        "id": "bSkRQ2Kom6Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "y = pd.Series(california.target, name='MedHouseVal')\n",
        "\n",
        "# Prepare the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "pRXHw8DJm4ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize missingness\n",
        "\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'png'\n",
        "\n",
        "msno.matrix(X_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SsWsLCV6xmTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R^2 Score: {r2}')\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PxJD_rtSzrlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression (Participant - on your own)\n",
        "# Instructions: Train a linear regression model using a subset of features such as 'MedInc' and 'HouseAge'.\n",
        "X_participant = X[['MedInc', 'HouseAge']]\n",
        "y_participant = y\n",
        "X_train_participant, X_test_participant, y_train_participant, y_test_participant = train_test_split(X_participant, y_participant, test_size=0.2, random_state=42)\n",
        "model_participant = LinearRegression()\n",
        "model_participant.fit(X_train_participant, y_train_participant)\n",
        "y_pred_participant = model_participant.predict(X_test_participant)\n",
        "mse_participant = mean_squared_error(y_test_participant, y_pred_participant)\n",
        "r2_participant = r2_score(y_test_participant, y_pred_participant)\n",
        "print(f'Mean Squared Error (Participant): {mse_participant}')\n",
        "print(f'R^2 Score (Participant): {r2_participant}')"
      ],
      "metadata": {
        "id": "xrIda4vcxugH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA"
      ],
      "metadata": {
        "id": "vVSjHPo-m75e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the breast _cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data=load_breast_cancer()\n",
        "data.keys()\n",
        "\n",
        "# Check the output classes\n",
        "print(data['target_names'])\n",
        "\n",
        "# Check the input attributes\n",
        "print(data['feature_names'])"
      ],
      "metadata": {
        "id": "Yqf-BEMem-tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a dataframe using pandas\n",
        "df1=pd.DataFrame(data['data'],columns=data['feature_names'])\n",
        "\n",
        "# Scale data before applying PCA\n",
        "scaling=StandardScaler()\n",
        "\n",
        "# Use fit and transform method\n",
        "scaling.fit(df1)\n",
        "Scaled_data=scaling.transform(df1)\n",
        "\n",
        "# Set the n_components=3\n",
        "principal=PCA(n_components=3)\n",
        "principal.fit(Scaled_data)\n",
        "x=principal.transform(Scaled_data)\n",
        "\n",
        "# Check the dimensions of data after PCA\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "hH2a0mg91DLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "principal.components_"
      ],
      "metadata": {
        "id": "rvCZMHhV1IS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(x[:,0],x[:,1],c=data['target'],cmap='plasma')\n",
        "plt.xlabel('pc1')\n",
        "plt.ylabel('pc2')"
      ],
      "metadata": {
        "id": "ZxjV1eL61Ki4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "\n",
        "# choose projection 3d for creating a 3d graph\n",
        "axis = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# x[:,0]is pc1,x[:,1] is pc2 while x[:,2] is pc3\n",
        "axis.scatter(x[:,0],x[:,1],x[:,2], c=data['target'],cmap='plasma')\n",
        "axis.set_xlabel(\"PC1\", fontsize=10)\n",
        "axis.set_ylabel(\"PC2\", fontsize=10)\n",
        "axis.set_zlabel(\"PC3\", fontsize=10)"
      ],
      "metadata": {
        "id": "4vMNU1911QLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check how much variance is explained by each principal component\n",
        "print(principal.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "2tkpJ-D91VMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering (K-means)"
      ],
      "metadata": {
        "id": "TSdZJgEGm-LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mpl_toolkits.mplot3d  # noqa: F401\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "8-6iJMbZn1LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The iris dataset has various measures related to n species of irises.\n",
        "# Without checking how many different species are present, can we run a\n",
        "# clustering algorithm to sort and differentiate these species of flowers?\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# K-means clustering is one way to do this (unsupervised ML)\n",
        "n = 5 # how many clusters you expect\n",
        "kmeans = KMeans(n_clusters=n, random_state=42) # make a model class\n",
        "kmeans.fit(X) # fit the model\n",
        "\n",
        "# pull the results from the model\n",
        "\n",
        "labels = kmeans.labels_ # cluster labels\n",
        "\n",
        "# plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(1, 1, 1, projection=\"3d\", elev=48, azim=134)\n",
        "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(float), edgecolor=\"k\")\n",
        "ax.xaxis.set_ticklabels([])\n",
        "ax.yaxis.set_ticklabels([])\n",
        "ax.zaxis.set_ticklabels([])\n",
        "ax.set_xlabel(\"Petal width\")\n",
        "ax.set_ylabel(\"Sepal length\")\n",
        "ax.set_zlabel(\"Petal length\")"
      ],
      "metadata": {
        "id": "iTE3IZQnvEr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A more principled way to select the number of clusters:\n",
        "\n",
        "# \"Inertia\" is the sum of squared distance of samples to their closest cluster center.\n",
        "# A good model is one with low inertia AND a low number of clusters ( K ).\n",
        "# However, this is a tradeoff because as K increases, inertia decreases.\n",
        "# To find the optimal K for a dataset, use the Elbow method; find the point\n",
        "# where the decrease in inertia begins to slow. K=3 is the “elbow” of this graph.\n",
        "\n",
        "# On your own: finish the for loop below to plot inertia over n_clusters,\n",
        "# and determine the optimal number of clusters\n",
        "\n",
        "inertia = []\n",
        "for n in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=n, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1, 11), inertia)\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal Number of Clusters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yvPs5Y2VuS0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare k-means output to ground truth\n",
        "\n",
        "np.random.seed(5)\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "estimators = [\n",
        "    (\"k_means_iris_8\", KMeans(n_clusters=8)),\n",
        "    (\"k_means_iris_3\", KMeans(n_clusters=3)),\n",
        "    (\"k_means_iris_bad_init\", KMeans(n_clusters=3, n_init=1, init=\"random\")),\n",
        "]\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "titles = [\"8 clusters\", \"3 clusters\", \"3 clusters, bad initialization\"]\n",
        "for idx, ((name, est), title) in enumerate(zip(estimators, titles)):\n",
        "    ax = fig.add_subplot(2, 2, idx + 1, projection=\"3d\", elev=48, azim=134)\n",
        "    est.fit(X)\n",
        "    labels = est.labels_\n",
        "\n",
        "    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(float), edgecolor=\"k\")\n",
        "\n",
        "    ax.xaxis.set_ticklabels([])\n",
        "    ax.yaxis.set_ticklabels([])\n",
        "    ax.zaxis.set_ticklabels([])\n",
        "    ax.set_xlabel(\"Petal width\")\n",
        "    ax.set_ylabel(\"Sepal length\")\n",
        "    ax.set_zlabel(\"Petal length\")\n",
        "    ax.set_title(title)\n",
        "\n",
        "# Plot the ground truth\n",
        "ax = fig.add_subplot(2, 2, 4, projection=\"3d\", elev=48, azim=134)\n",
        "\n",
        "for name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n",
        "    ax.text3D(\n",
        "        X[y == label, 3].mean(),\n",
        "        X[y == label, 0].mean(),\n",
        "        X[y == label, 2].mean() + 2,\n",
        "        name,\n",
        "        horizontalalignment=\"center\",\n",
        "        bbox=dict(alpha=0.2, edgecolor=\"w\", facecolor=\"w\"),\n",
        "    )\n",
        "\n",
        "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\"k\")\n",
        "\n",
        "ax.xaxis.set_ticklabels([])\n",
        "ax.yaxis.set_ticklabels([])\n",
        "ax.zaxis.set_ticklabels([])\n",
        "ax.set_xlabel(\"Petal width\")\n",
        "ax.set_ylabel(\"Sepal length\")\n",
        "ax.set_zlabel(\"Petal length\")\n",
        "ax.set_title(\"Ground Truth\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.25, hspace=0.25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uRAKNfW5uS69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deeper Dive into ML"
      ],
      "metadata": {
        "id": "Z_n8UCYdnL7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression with Titanic Dataset"
      ],
      "metadata": {
        "id": "GDMoCghJnQpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic"
      ],
      "metadata": {
        "id": "SATX5cqw3Pew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in np.ndindex(cm.shape):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ],
      "metadata": {
        "id": "w1ti92jL2Qiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "titanic = titanic.dropna(subset=['age', 'fare', 'embarked'])\n",
        "\n",
        "# Prepare the data\n",
        "X = pd.get_dummies(titanic[['age', 'fare', 'sex', 'embarked', 'class', 'who', 'alone']], drop_first=True)\n",
        "y = titanic['survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logreg.predict(X_test_scaled)\n",
        "y_prob = logreg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, classes=['Not Survived', 'Survived'],\n",
        "                      title='Confusion Matrix - Initial Model')\n",
        "\n"
      ],
      "metadata": {
        "id": "4qFgnR9q3kzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Improving the model with feature engineering and hyperparameter tuning\n",
        "# Adding interaction terms\n",
        "X['age_fare'] = X['age'] * X['fare']\n",
        "\n",
        "# Prepare the data again with the new feature\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'solver': ['liblinear', 'lbfgs']}\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best model from GridSearchCV\n",
        "best_logreg = grid_search.best_estimator_\n",
        "y_pred_best = best_logreg.predict(X_test_scaled)\n",
        "y_prob_best = best_logreg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluate the improved model\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "cm_best = confusion_matrix(y_test, y_pred_best)\n",
        "fpr_best, tpr_best, _ = roc_curve(y_test, y_prob_best)\n",
        "roc_auc_best = auc(fpr_best, tpr_best)\n",
        "\n",
        "print(f'Improved Accuracy: {accuracy_best}')\n",
        "print(f'Improved Confusion Matrix:\\n{cm_best}')\n",
        "\n",
        "# Plot ROC Curve for the improved model\n",
        "plt.figure()\n",
        "plt.plot(fpr_best, tpr_best, color='darkorange', lw=2, label=f'Improved ROC curve (area = {roc_auc_best:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (Improved)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Plot the confusion matrix for the improved model\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm_best, classes=['Not Survived', 'Survived'],\n",
        "                      title='Confusion Matrix - Improved Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hr-a5km_3fDu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}